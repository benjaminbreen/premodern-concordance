{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune BGE-M3 for Premodern Concordance\n",
    "\n",
    "This notebook fine-tunes BGE-M3 on historical entity pairs using Google Colab's free GPU.\n",
    "\n",
    "**IMPORTANT: Uses proper train/test split to avoid overfitting.**\n",
    "- 75% of pairs used for training\n",
    "- 25% held out for evaluation (never seen during training)\n",
    "\n",
    "**Instructions:**\n",
    "1. Go to Runtime → Change runtime type → Select GPU (T4)\n",
    "2. Upload `combined_pairs.csv` when prompted\n",
    "3. Run all cells\n",
    "4. Download the fine-tuned model at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q sentence-transformers pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Go to Runtime → Change runtime type → GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your combined_pairs.csv\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Link type categories\n",
    "POSITIVE_LINK_TYPES = {\"same_referent\", \"orthographic_variant\", \"derivation\"}\n",
    "NEGATIVE_LINK_TYPES = {\"hard_negative\"}\n",
    "UNCERTAIN_LINK_TYPES = {\"conceptual_overlap\", \"contested_identity\"}\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('combined_pairs.csv')\n",
    "print(f\"Loaded {len(df)} pairs\")\n",
    "print()\n",
    "print(\"Distribution by link type:\")\n",
    "print(df['link_type'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL: Split data BEFORE training to avoid overfitting\n",
    "TEST_SIZE = 0.25\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    df, \n",
    "    test_size=TEST_SIZE, \n",
    "    random_state=RANDOM_STATE, \n",
    "    stratify=df['link_type']  # Ensure each link type is represented in both sets\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(train_df)} pairs\")\n",
    "print(f\"Test set (HELD OUT): {len(test_df)} pairs\")\n",
    "print()\n",
    "print(\"Test set distribution:\")\n",
    "print(test_df['link_type'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data (ONLY from train_df)\n",
    "train_examples = []\n",
    "\n",
    "for _, row in train_df.iterrows():\n",
    "    if row['link_type'] in POSITIVE_LINK_TYPES:\n",
    "        train_examples.append(InputExample(texts=[row['term_a'], row['term_b']], label=1.0))\n",
    "    elif row['link_type'] in NEGATIVE_LINK_TYPES:\n",
    "        train_examples.append(InputExample(texts=[row['term_a'], row['term_b']], label=0.0))\n",
    "    elif row['link_type'] in UNCERTAIN_LINK_TYPES:\n",
    "        label = 0.5 if row['link_type'] == 'conceptual_overlap' else 0.3\n",
    "        train_examples.append(InputExample(texts=[row['term_a'], row['term_b']], label=label))\n",
    "\n",
    "# Data augmentation: add swapped pairs\n",
    "augmented = [InputExample(texts=[ex.texts[1], ex.texts[0]], label=ex.label) for ex in train_examples]\n",
    "train_examples.extend(augmented)\n",
    "random.shuffle(train_examples)\n",
    "\n",
    "print(f\"Training examples (with augmentation): {len(train_examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "print(\"Loading BGE-M3 (this may take a minute)...\")\n",
    "model = SentenceTransformer('BAAI/bge-m3')\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "LR = 2e-5\n",
    "WARMUP_STEPS = 10\n",
    "OUTPUT_PATH = 'finetuned-bge-m3-premodern'\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=BATCH_SIZE)\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "steps_per_epoch = len(train_dataloader)\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"Total epochs: {EPOCHS}\")\n",
    "print(f\"Total steps: {steps_per_epoch * EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "print(\"Starting training...\")\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=EPOCHS,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    output_path=OUTPUT_PATH,\n",
    "    optimizer_params={'lr': LR},\n",
    "    show_progress_bar=True,\n",
    ")\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on HELD-OUT test set\n",
    "print(\"=\"*60)\n",
    "print(\"EVALUATION ON HELD-OUT TEST SET\")\n",
    "print(\"(These pairs were NEVER seen during training)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def cosine(a, b):\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
    "\n",
    "# Load both models for comparison\n",
    "original_model = SentenceTransformer('BAAI/bge-m3')\n",
    "finetuned_model = SentenceTransformer(OUTPUT_PATH)\n",
    "\n",
    "# Evaluate on test set\n",
    "results = []\n",
    "for _, row in test_df.iterrows():\n",
    "    orig_sim = cosine(original_model.encode(row['term_a']), original_model.encode(row['term_b']))\n",
    "    ft_sim = cosine(finetuned_model.encode(row['term_a']), finetuned_model.encode(row['term_b']))\n",
    "    is_match = row['link_type'] != 'hard_negative'\n",
    "    results.append({\n",
    "        'term_a': row['term_a'],\n",
    "        'term_b': row['term_b'],\n",
    "        'link_type': row['link_type'],\n",
    "        'original_sim': orig_sim,\n",
    "        'finetuned_sim': ft_sim,\n",
    "        'delta': ft_sim - orig_sim,\n",
    "        'is_match': is_match,\n",
    "        'orig_pred': orig_sim >= 0.7,\n",
    "        'ft_pred': ft_sim >= 0.7,\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "print(\"\\nOVERALL METRICS (threshold=0.7):\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "for name, pred_col in [('Original BGE-M3', 'orig_pred'), ('Fine-tuned', 'ft_pred')]:\n",
    "    y_true = results_df['is_match'].to_numpy()\n",
    "    y_pred = results_df[pred_col].to_numpy()\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}\")\n",
    "\n",
    "print(\"\\nPER LINK-TYPE RESULTS:\")\n",
    "print(\"-\"*40)\n",
    "for link_type in sorted(results_df['link_type'].unique()):\n",
    "    subset = results_df[results_df['link_type'] == link_type]\n",
    "    orig_avg = subset['original_sim'].mean()\n",
    "    ft_avg = subset['finetuned_sim'].mean()\n",
    "    delta = ft_avg - orig_avg\n",
    "    orig_recall = (subset['original_sim'] >= 0.7).mean()\n",
    "    ft_recall = (subset['finetuned_sim'] >= 0.7).mean()\n",
    "    print(f\"{link_type}:\")\n",
    "    print(f\"  Avg sim: {orig_avg:.3f} → {ft_avg:.3f} ({delta:+.3f})\")\n",
    "    print(f\"  Recall@0.7: {orig_recall:.1%} → {ft_recall:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show biggest improvements\n",
    "print(\"\\nTOP 10 IMPROVEMENTS (fine-tuned vs original):\")\n",
    "print(\"-\"*60)\n",
    "improvements = results_df[results_df['is_match']].nlargest(10, 'delta')\n",
    "for _, row in improvements.iterrows():\n",
    "    print(f\"{row['term_a']} / {row['term_b']} ({row['link_type']})\")\n",
    "    print(f\"  {row['original_sim']:.3f} → {row['finetuned_sim']:.3f} ({row['delta']:+.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for potential problems: did hard negatives get pushed DOWN?\n",
    "print(\"\\nHARD NEGATIVE SEPARATION CHECK:\")\n",
    "print(\"-\"*40)\n",
    "hard_neg = results_df[results_df['link_type'] == 'hard_negative']\n",
    "same_ref = results_df[results_df['link_type'] == 'same_referent']\n",
    "\n",
    "if len(hard_neg) > 0 and len(same_ref) > 0:\n",
    "    orig_gap = same_ref['original_sim'].mean() - hard_neg['original_sim'].mean()\n",
    "    ft_gap = same_ref['finetuned_sim'].mean() - hard_neg['finetuned_sim'].mean()\n",
    "    print(f\"Gap (same_referent - hard_negative):\")\n",
    "    print(f\"  Original: {orig_gap:.3f}\")\n",
    "    print(f\"  Fine-tuned: {ft_gap:.3f}\")\n",
    "    if ft_gap > orig_gap:\n",
    "        print(f\"  ✓ Improved separation by {ft_gap - orig_gap:.3f}\")\n",
    "    else:\n",
    "        print(f\"  ⚠ Separation decreased by {orig_gap - ft_gap:.3f}\")\n",
    "else:\n",
    "    print(\"Not enough hard_negative or same_referent pairs in test set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_df.to_csv('test_results.csv', index=False)\n",
    "print(\"Saved test results to test_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the fine-tuned model\n",
    "!zip -r finetuned-model.zip {OUTPUT_PATH}\n",
    "files.download('finetuned-model.zip')\n",
    "files.download('test_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
