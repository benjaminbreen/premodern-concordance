{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Fine-tune Qwen3-0.6B for Entity Extraction\n",
    "\n",
    "This notebook fine-tunes Qwen3-0.6B on early modern text entity extraction using LoRA.\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab with T4 GPU (free tier works)\n",
    "- Upload `entity_training_data.jsonl` from your local machine\n",
    "\n",
    "**Time:** ~1-2 hours on T4"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Install Dependencies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "!pip install unsloth\n",
    "!pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --upgrade torch\n",
    "!pip install triton xformers trl peft accelerate bitsandbytes"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Upload Training Data\n",
    "\n",
    "Upload your `entity_training_data.jsonl` file when prompted."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import files\n",
    "import json\n",
    "\n",
    "# Upload the training data file\n",
    "print(\"Please upload entity_training_data.jsonl\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Load and verify\n",
    "filename = list(uploaded.keys())[0]\n",
    "with open(filename, 'r') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"\\nLoaded {len(data)} training examples\")\n",
    "print(f\"\\nSample example:\")\n",
    "print(json.dumps(data[0], indent=2)[:500] + \"...\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Load Qwen3-0.6B with Unsloth"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Model configuration\n",
    "max_seq_length = 4096\n",
    "dtype = None  # Auto-detect\n",
    "load_in_4bit = True  # Use 4-bit quantization for memory efficiency\n",
    "\n",
    "# Load Qwen3-0.6B\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Qwen3-0.6B\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {model.config._name_or_path}\")\n",
    "print(f\"Parameters: {model.num_parameters():,}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Configure LoRA"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Add LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=64,  # LoRA rank\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"LoRA adapters added\")\n",
    "model.print_trainable_parameters()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Prepare Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from datasets import Dataset\nimport random\n\ndef format_chat(example):\n    \"\"\"Format messages into a single training string.\"\"\"\n    messages = example['messages']\n    \n    # Build the prompt\n    text = \"\"\n    for msg in messages:\n        role = msg['role']\n        content = msg['content']\n        if role == 'system':\n            text += f\"<|im_start|>system\\n{content}<|im_end|>\\n\"\n        elif role == 'user':\n            text += f\"<|im_start|>user\\n{content}<|im_end|>\\n\"\n        elif role == 'assistant':\n            text += f\"<|im_start|>assistant\\n{content}<|im_end|>\\n\"\n    \n    return {\"text\": text}\n\n# Shuffle and split data (90% train, 10% test)\nrandom.seed(42)\nshuffled_data = data.copy()\nrandom.shuffle(shuffled_data)\n\nsplit_idx = int(len(shuffled_data) * 0.9)\ntrain_data = shuffled_data[:split_idx]\ntest_data = shuffled_data[split_idx:]\n\nprint(f\"Total examples: {len(data)}\")\nprint(f\"Training set: {len(train_data)} examples (90%)\")\nprint(f\"Test set: {len(test_data)} examples (10%)\")\n\n# Save test set for evaluation after training\nwith open(\"test_set.json\", \"w\") as f:\n    json.dump(test_data, f, ensure_ascii=False, indent=2)\nprint(f\"\\nTest set saved to test_set.json for post-training evaluation\")\n\n# Create training dataset (only train on train_data, NOT full data)\ndataset = Dataset.from_list(train_data)\ndataset = dataset.map(format_chat)\n\nprint(f\"\\nTraining dataset size: {len(dataset)}\")\nprint(f\"\\nSample formatted text (first 500 chars):\")\nprint(dataset[0]['text'][:500])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=10,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=42,\n",
    "        output_dir=\"outputs\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer_stats = trainer.train()\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"Training time: {trainer_stats.metrics['train_runtime']:.0f} seconds\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Evaluate on Held-Out Test Set\n\nRun the fine-tuned model on examples it has never seen during training.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Evaluate on held-out test set\nFastLanguageModel.for_inference(model)\n\n# Load test set\nwith open(\"test_set.json\", \"r\") as f:\n    test_examples = json.load(f)\n\nprint(f\"Evaluating on {len(test_examples)} held-out test examples...\\n\")\n\ndef extract_entities_from_response(response_text):\n    \"\"\"Parse JSON entities from model response.\"\"\"\n    try:\n        match = re.search(r'\\[.*\\]', response_text, re.DOTALL)\n        if match:\n            return json.loads(match.group())\n    except:\n        pass\n    return []\n\nimport re\nresults = []\nfor i, example in enumerate(test_examples[:10]):  # Evaluate first 10 for speed\n    # Get the passage from user message\n    user_msg = example['messages'][1]['content']\n    passage = user_msg.replace(\"Extract all named entities from this passage:\\n\\n\", \"\")\n    \n    # Get expected entities\n    expected = json.loads(example['messages'][2]['content'])\n    \n    # Generate prediction\n    prompt = f\"\"\"<|im_start|>system\nYou extract named entities from early modern European texts (1500-1800).\nReturn a JSON array with name, category (PERSON/SUBSTANCE/CONCEPT), and brief context.<|im_end|>\n<|im_start|>user\nExtract all named entities from this passage:\n\n{passage}<|im_end|>\n<|im_start|>assistant\n\"\"\"\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n    outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.1, pad_token_id=tokenizer.eos_token_id)\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Extract predicted entities\n    if \"<|im_start|>assistant\" in response:\n        pred_text = response.split(\"<|im_start|>assistant\")[-1].strip()\n    else:\n        pred_text = response\n    \n    predicted = extract_entities_from_response(pred_text)\n    \n    # Calculate metrics\n    expected_names = set(e['name'].lower() for e in expected)\n    predicted_names = set(e['name'].lower() for e in predicted)\n    \n    overlap = expected_names & predicted_names\n    precision = len(overlap) / len(predicted_names) if predicted_names else 0\n    recall = len(overlap) / len(expected_names) if expected_names else 0\n    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n    \n    results.append({\n        'expected': len(expected),\n        'predicted': len(predicted),\n        'overlap': len(overlap),\n        'precision': precision,\n        'recall': recall,\n        'f1': f1\n    })\n    \n    print(f\"Example {i+1}: Expected {len(expected)}, Predicted {len(predicted)}, Overlap {len(overlap)}, F1={f1:.2f}\")\n\n# Summary\navg_precision = sum(r['precision'] for r in results) / len(results)\navg_recall = sum(r['recall'] for r in results) / len(results)\navg_f1 = sum(r['f1'] for r in results) / len(results)\n\nprint(f\"\\n{'='*50}\")\nprint(f\"EVALUATION RESULTS (on {len(results)} held-out examples)\")\nprint(f\"{'='*50}\")\nprint(f\"Average Precision: {avg_precision:.2%}\")\nprint(f\"Average Recall: {avg_recall:.2%}\")\nprint(f\"Average F1 Score: {avg_f1:.2%}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. Save the Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Save LoRA adapters\n",
    "model.save_pretrained(\"qwen3-0.6b-entities-lora\")\n",
    "tokenizer.save_pretrained(\"qwen3-0.6b-entities-lora\")\n",
    "\n",
    "print(\"LoRA adapters saved to qwen3-0.6b-entities-lora/\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Option A: Save merged model (larger but easier to use)\n",
    "model.save_pretrained_merged(\n",
    "    \"qwen3-0.6b-entities-merged\",\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\",\n",
    ")\n",
    "print(\"Merged model saved to qwen3-0.6b-entities-merged/\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Option B: Save as GGUF for ollama (recommended for local use)\n",
    "model.save_pretrained_gguf(\n",
    "    \"qwen3-0.6b-entities-gguf\",\n",
    "    tokenizer,\n",
    "    quantization_method=\"q4_k_m\",  # Good balance of size/quality\n",
    ")\n",
    "print(\"GGUF model saved to qwen3-0.6b-entities-gguf/\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 9. Download the Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Zip and download the GGUF model for use with ollama\n",
    "!zip -r qwen3-0.6b-entities-gguf.zip qwen3-0.6b-entities-gguf/\n",
    "\n",
    "from google.colab import files\n",
    "files.download('qwen3-0.6b-entities-gguf.zip')\n",
    "\n",
    "print(\"\\nDownload complete! To use with ollama:\")\n",
    "print(\"1. Unzip the file\")\n",
    "print(\"2. Create a Modelfile with: FROM ./qwen3-0.6b-entities-gguf/unsloth.Q4_K_M.gguf\")\n",
    "print(\"3. Run: ollama create qwen3-entities -f Modelfile\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Done!\n",
    "\n",
    "You now have a fine-tuned Qwen3-0.6B model optimized for extracting entities from early modern texts.\n",
    "\n",
    "**Next steps:**\n",
    "1. Download the GGUF file\n",
    "2. Import into ollama\n",
    "3. Test against Gemini 2.5 Flash Lite and GPT-5 Nano\n",
    "4. Run on full Semedo text"
   ],
   "metadata": {}
  }
 ]
}